{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1631e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Libraries imported and model architecture defined\n"
     ]
    }
   ],
   "source": [
    "# === Environmental Setup and Model Architecture ===\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (cuda or cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom RoBERTa with ordinal classification for truthfulness scoring\n",
    "class RobertaOrdinalClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses RoBERTa, a pre-trained and optimized version of META's Bert Model.\n",
    "    Predefines classificaiton heads for 6 output classes (pants-fire -> true)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=6, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Helper function for unified probability output\n",
    "def compute_truthfulness_score(class_probs):\n",
    "    \"\"\"\n",
    "    Convert multi-class probabilities to continuous truthfulness score (0.0 to 1.0)\n",
    "    Calculates expected value of each classification confidence to produce final unified score\n",
    "    \"\"\"\n",
    "    class_values = torch.tensor([0.0, 0.2, 0.4, 0.6, 0.8, 1.0], device=class_probs.device)\n",
    "    truthfulness = torch.sum(class_probs * class_values, dim=1)\n",
    "    return truthfulness\n",
    "\n",
    "print(\"Libraries imported and model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fa8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1267 test samples\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "barely-true    212\n",
      "false          249\n",
      "half-true      265\n",
      "mostly-true    241\n",
      "pants-fire      92\n",
      "true           208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "SAMPLE STATEMENTS\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "  Label: mostly-true\n",
      "  Statement: The fact of the matter is that my colleague from New York, Senator Clinton, there are 50 percent of the American public that say they're not going to vote for her.\n",
      "  Subject: elections\n",
      "  Context: a debate in Philadelphia\n",
      "\n",
      "Example 2:\n",
      "  Label: false\n",
      "  Statement: In the U.S. Constitution, theres a little section in there that talks about life, liberty and the pursuit of happiness.\n",
      "  Subject: history\n",
      "  Context: in a speech announcing his presidential run\n"
     ]
    }
   ],
   "source": [
    "# === Dataset Loading and Preperation ===\n",
    "\n",
    "def load_liar_data(file_path):\n",
    "    \"\"\" Loads LIAR dataset, preselecting labels, statements, subjects, and context columns\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        usecols=[1, 2, 3, 13],\n",
    "        names=['label', 'statement', 'subject', 'context']\n",
    "    )\n",
    "    \n",
    "    # Clean text columns for uniformity\n",
    "    for col in ['statement', 'subject', 'context']:\n",
    "        df[col] = df[col].fillna('').astype(str).str.strip()\n",
    "    \n",
    "    # Map labels to scalar quantities\n",
    "    label_map = {\n",
    "        'pants-fire': 0,\n",
    "        'false': 1,\n",
    "        'barely-true': 2,\n",
    "        'half-true': 3,\n",
    "        'mostly-true': 4,\n",
    "        'true': 5\n",
    "    }\n",
    "    df['label_encoded'] = df['label'].map(label_map)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load test dataset\n",
    "test_df = load_liar_data('liar_dataset/test.tsv')\n",
    "print(f\"Loaded {len(test_df)} test samples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(test_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE STATEMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Randomly select two examples\n",
    "sample_indices = list(np.random.choice(test_df.index, size=2, replace=False))\n",
    "\n",
    "# Display examples\n",
    "for i, idx in enumerate(sample_indices[:2], 1):\n",
    "    row = test_df.iloc[idx]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Label: {row['label']}\")\n",
    "    print(f\"  Statement: {row['statement']}\")\n",
    "    print(f\"  Subject: {row['subject']}\")\n",
    "    print(f\"  Context: {row['context']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and set to evaluation mode\n"
     ]
    }
   ],
   "source": [
    "# === Tokenizer and Model Preperation ===\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "# Load trained model\n",
    "model = RobertaOrdinalClassifier(num_classes=6, dropout=0.3)\n",
    "model.load_state_dict(torch.load('liar_model.pt', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded and set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fb64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Inference and Data Collection ===\n",
    "\n",
    "def predict_batch(model, statements, subjects, contexts, tokenizer, device, batch_size=32):\n",
    "    \"\"\" Run inference, statements are batched to increase throughput \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    all_truthfulness_scores = []\n",
    "    all_predicted_classes = []\n",
    "    all_class_probabilities = []\n",
    "    \n",
    "    num_batches = (len(statements) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_batches), desc=\"Running inference\"):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(statements))\n",
    "            \n",
    "            batch_statements = statements[start_idx:end_idx]\n",
    "            batch_subjects = subjects[start_idx:end_idx]\n",
    "            batch_contexts = contexts[start_idx:end_idx]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            input_texts = []\n",
    "            for stmt, subj, ctx in zip(batch_statements, batch_subjects, batch_contexts):\n",
    "                if subj and ctx:\n",
    "                    input_texts.append(f\"{stmt} {tokenizer.sep_token} {subj} {tokenizer.sep_token} {ctx}\")\n",
    "                elif subj:\n",
    "                    input_texts.append(f\"{stmt} {tokenizer.sep_token} {subj}\")\n",
    "                else:\n",
    "                    input_texts.append(stmt)\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encoding = tokenizer(\n",
    "                input_texts,\n",
    "                max_length=256,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Data to compute device\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Compute truthfulness scores\n",
    "            truthfulness = compute_truthfulness_score(probs)\n",
    "            \n",
    "            # Get predicted classes\n",
    "            predicted = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            # Store results\n",
    "            all_truthfulness_scores.extend(truthfulness.cpu().numpy())\n",
    "            all_predicted_classes.extend(predicted.cpu().numpy())\n",
    "            all_class_probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_truthfulness_scores), np.array(all_predicted_classes), np.array(all_class_probabilities)\n",
    "\n",
    "# Run inference on test set\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "truthfulness_scores, predicted_classes, class_probabilities = predict_batch(\n",
    "    model=model,\n",
    "    statements=test_df['statement'].values,\n",
    "    subjects=test_df['subject'].values,\n",
    "    contexts=test_df['context'].values,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Add results to dataframe\n",
    "test_df['truthfulness_score'] = truthfulness_scores\n",
    "test_df['predicted_class'] = predicted_classes\n",
    "\n",
    "class_names = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']\n",
    "test_df['predicted_label'] = test_df['predicted_class'].map(lambda x: class_names[x])\n",
    "\n",
    "print(\"Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bef890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Truthfulness Score Statistics:\n",
      "  Mean: 0.514\n",
      "  Std Dev: 0.187\n",
      "  Min: 0.041\n",
      "  Max: 0.880\n",
      "  Median: 0.546\n",
      "\n",
      "\n",
      "Truthfulness Score by True Label:\n",
      "True Label      Mean Score   Std Dev    Count   \n",
      "--------------------------------------------------\n",
      "pants-fire            0.320      0.218       92\n",
      "false                 0.466      0.180      249\n",
      "barely-true           0.472      0.172      212\n",
      "half-true             0.519      0.171      265\n",
      "mostly-true           0.599      0.144      241\n",
      "true                  0.595      0.165      208\n",
      "\n",
      "================================================================================\n",
      "SAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "  Statement: When asked by a reporter whether hes at the center of a criminal scheme to violate campaign laws, Gov. Scott Walker nodded yes....\n",
      "  Subject: campaign-finance,legal-issues,campaign-advertising\n",
      "  True Label: pants-fire\n",
      "  Predicted Label: false\n",
      "  Truthfulness Score: 0.487\n",
      "  Class Probabilities:\n",
      "    pants-fire      0.111 ██\n",
      "    false           0.310 ██████\n",
      "    barely-true     0.117 ██\n",
      "    half-true       0.148 ██\n",
      "    mostly-true     0.124 ██\n",
      "    true            0.190 ███\n",
      "\n",
      "Example 2:\n",
      "  Statement: Wisconsin is on pace to double the number of layoffs this year....\n",
      "  Subject: jobs\n",
      "  True Label: false\n",
      "  Predicted Label: half-true\n",
      "  Truthfulness Score: 0.564\n",
      "  Class Probabilities:\n",
      "    pants-fire      0.053 █\n",
      "    false           0.162 ███\n",
      "    barely-true     0.127 ██\n",
      "    half-true       0.334 ██████\n",
      "    mostly-true     0.221 ████\n",
      "    true            0.104 ██\n",
      "\n",
      "================================================================================\n",
      "ACCURACY METRICS\n",
      "================================================================================\n",
      "\n",
      "Exact Match Accuracy: 0.2794 (27.94%)\n",
      "Within ±1 Class: 0.6125 (61.25%)\n",
      "Within ±2 Classes: 0.7987 (79.87%)\n",
      "\n",
      "\n",
      "Distribution of Predictions by Score Range:\n",
      "  Highly False    [0.0-0.2]:  103 (  8.1%) ████\n",
      "  Likely False    [0.2-0.4]:  225 ( 17.8%) ████████\n",
      "  Uncertain       [0.4-0.6]:  445 ( 35.1%) █████████████████\n",
      "  Likely True     [0.6-0.8]:  476 ( 37.6%) ██████████████████\n",
      "  Highly True     [0.8-1.0]:   18 (  1.4%) \n",
      "\n",
      "Results saved to 'test_predictions_with_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "# === Inference Results ===\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\nTruthfulness Score Statistics:\")\n",
    "print(f\"  Mean: {truthfulness_scores.mean():.3f}\")\n",
    "print(f\"  Std Dev: {truthfulness_scores.std():.3f}\")\n",
    "print(f\"  Min: {truthfulness_scores.min():.3f}\")\n",
    "print(f\"  Max: {truthfulness_scores.max():.3f}\")\n",
    "print(f\"  Median: {np.median(truthfulness_scores):.3f}\")\n",
    "\n",
    "# Score distribution by true label\n",
    "print(f\"\\n\\nTruthfulness Score by True Label:\")\n",
    "print(f\"{'True Label':<15} {'Mean Score':<12} {'Std Dev':<10} {'Count':<8}\")\n",
    "print(\"-\" * 50)\n",
    "for label in class_names:\n",
    "    mask = test_df['label'] == label\n",
    "    if mask.sum() > 0:\n",
    "        scores = test_df[mask]['truthfulness_score']\n",
    "        print(f\"{label:<15} {scores.mean():>11.3f} {scores.std():>10.3f} {mask.sum():>8}\")\n",
    "\n",
    "# Show examples with predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show the same examples we displayed earlier\n",
    "for i, idx in enumerate(sample_indices[:2], 1):\n",
    "    row = test_df.loc[idx]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Statement: {row['statement'][:150]}...\")\n",
    "    print(f\"  Subject: {row['subject']}\")\n",
    "    print(f\"  True Label: {row['label']}\")\n",
    "    print(f\"  Predicted Label: {row['predicted_label']}\")\n",
    "    print(f\"  Truthfulness Score: {row['truthfulness_score']:.3f}\")\n",
    "    print(f\"  Class Probabilities:\")\n",
    "    probs = class_probabilities[idx]\n",
    "    for class_idx, (class_name, prob) in enumerate(zip(class_names, probs)):\n",
    "        bar = '█' * int(prob * 20)\n",
    "        print(f\"    {class_name:15s} {prob:.3f} {bar}\")\n",
    "\n",
    "# Accuracy metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACCURACY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Exact accuracy\n",
    "exact_accuracy = (test_df['predicted_class'] == test_df['label_encoded']).mean()\n",
    "print(f\"\\nExact Match Accuracy: {exact_accuracy:.4f} ({exact_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Off-by-one accuracy\n",
    "off_by_1 = (np.abs(test_df['predicted_class'] - test_df['label_encoded']) <= 1).mean()\n",
    "print(f\"Within ±1 Class: {off_by_1:.4f} ({off_by_1*100:.2f}%)\")\n",
    "\n",
    "# Off-by-two accuracy\n",
    "off_by_2 = (np.abs(test_df['predicted_class'] - test_df['label_encoded']) <= 2).mean()\n",
    "print(f\"Within ±2 Classes: {off_by_2:.4f} ({off_by_2*100:.2f}%)\")\n",
    "\n",
    "# Show distribution across score ranges\n",
    "print(\"\\n\\nDistribution of Predictions by Score Range:\")\n",
    "ranges = [(0.0, 0.2, \"Highly False\"), (0.2, 0.4, \"Likely False\"), \n",
    "          (0.4, 0.6, \"Uncertain\"), (0.6, 0.8, \"Likely True\"), \n",
    "          (0.8, 1.0, \"Highly True\")]\n",
    "\n",
    "for low, high, label in ranges:\n",
    "    count = ((truthfulness_scores >= low) & (truthfulness_scores < high)).sum()\n",
    "    pct = count / len(truthfulness_scores) * 100\n",
    "    bar = '█' * int(pct / 2)\n",
    "    print(f\"  {label:15s} [{low:.1f}-{high:.1f}]: {count:4d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "# Save results\n",
    "output_file = 'test_predictions_with_scores.csv'\n",
    "test_df[['statement', 'subject', 'context', 'label', 'predicted_label', \n",
    "         'truthfulness_score']].to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to '{output_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4371",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
